<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>


  <title>Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction</title>
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>
  
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Read to Play (R2-Play): <br> Decision Transformer with <br> Multimodal Game Instruction</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">Yonggang Jin</a><sup>2*</sup>,</span>
                <span class="author-block">Ge Zhang</a><sup>145*^</sup>,</span>
                  <span class="author-block">Hao Zhao</a><sup>2*</sup>,</span>
                  <span class="author-block">Tianyu Zheng</a><sup>2</sup>,</span>
                <span class="author-block">Jiawei Guo</a><sup>2</sup>,</span>
                  <span class="author-block">Liuyu Xiang</a><sup>2</sup>,</span>
                  <span class="author-block">Shawn Yue</a><sup>6</sup>,</span>
                <span class="author-block">Stephen W. Huang</a><sup>6</sup>,</span>
                  <span class="author-block">Wenhu Chen</a><sup>1,4,5</sup>,</span>
                  <span class="author-block">Zhaofeng He</a><sup>2^</sup>,</span>
                  <span class="author-block">Jie Fu</a><sup>1,3^</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Multimodal Art Projection Research Community;<br><sup>2</sup>Beijing University of Posts and Telecommunications;<br><sup>3</sup>HKUST; <sup>4</sup>University of Waterloo; <sup>5</sup>Vector Institute; <sup>6</sup>Harmony.AI;</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                    <span class="eql-cntrb"><small><br><sup>^</sup>Indicates Corresponding Authors</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2402.04154.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>
                      
                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ygjin11/R2-play" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code and Data</span>
                  </a>
                </span>
                      
                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Developing a generalist agent is a longstanding objective in artificial intelligence. Previous efforts utilizing extensive offline datasets from various tasks demonstrate remarkable performance in multitasking scenarios within Reinforcement Learning. However, these works encounter challenges in extending their capabilities to new tasks. Recent approaches integrate textual guidance or visual trajectory into decision networks to provide task-specific contextual cues, representing a promising direction. However, it is observed that relying solely on textual guidance or visual trajectory is insufficient for accurately conveying the contextual information of tasks. This paper explores enhanced forms of task guidance for agents, enabling them to comprehend gameplay instructions, thereby facilitating a ``read-to-play'' capability. Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task and construct a set of multimodal game instructions to incorporate instruction tuning into a decision transformer. Experimental results demonstrate that incorporating multimodal game instructions significantly enhances the decision transformer's multitasking and generalization capabilities. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper introduction -->
<section class="section hero is-white">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Creating a generalist agent that can accomplish diverse tasks is an enduring goal in artificial intelligence. The recent advancement in integrating textual guidance or visual trajectory into a single decision-making agent presents a potential solution. This line of research provides task-specific context to guide the agent. Although textual guidance and visual trajectory each offer advantages, they also have distinct limitations: (1) Textual guidance lacks visually-grounded information, which diminishes its expressiveness for decision-making tasks based on visual observations; (2) Without clear task instructions, deriving an effective strategy from a visual trajectory is extremely difficult, which is similar to people's difficulty understanding player intentions when watching game videos without explanations. The complementary relationship between textual guidance and visual trajectory suggests their combination enhances guidance effectiveness, as illustrated in Figure 1.  As a result, this paper aims to develop an agent capable of adapting to new tasks through multimodal guidance.  <br>  Similar endeavors are undertaken in the field of multimodal models. Drawing inspiration from the success of multimodal instruction tuning in visual tasks, we treat the visual-based RL task as a long-horizon vision task, aiming to integrate it into the RL field. We construct a set of Multimodal Game Instruction (MGI) to provide multimodal guidance for agents. The MGI set comprises thousands of game instructions sourced from approximately 50 diverse Atari games, designed to provide a detailed and thorough context. Each instruction entails a 20-step trajectory, labeled with corresponding textual language guidance. The construction of this multimodal game instruction set aims to empower agents to read game instructions for playing various games and adapting to the new ones.<br><br> 
          </p>
         <div class="item">
          <!-- Your image here -->
           <div style="text-align: center;">
          <img src="static/images/teaser.jpg" alt="MY ALT TEXT" style="width: 75%; margin: auto; display: block;"/>
           <div style="width: 75%; text-align: center; margin: 0 auto;">
            Figure1: Imagine an agent learning to play Palworld (a Pok\'emon-like game). (1) The agent exhibits confusion when only relying on textual guidance. (2) The agent is confused when presented with images of a Pal sphere and a Pal. (3) The agent understands how to catch a pet through multimodal guidance, which combines textual guidance with images of the Pal sphere and Pal. 
          </div>
          </div>
        </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper introduction -->

<!-- Paper Method -->
<section class="section hero is-white">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Game Instruction Construction</h2>
        <div class="content has-text-justified">
          <p>
            In the multimodal community, considerable attention is devoted to the integration of instruction tuning within multimodal models. These efforts aim to enhance the performance of multimodal models in performing specific visual tasks by incorporating language instructions. Drawing upon the insights derived from these efforts, it is imperative to explore the potential of leveraging multimodal game instructions to augment the capabilities of RL agents, especially in the context of visual-based RL tasks as long-horizon visual tasks. We construct a set of Multimodal Game Instruction (MGI) to apply the benefits of instruction tuning to DT. An illustrative example of a multimodal game instruction is presented in Figure 2.<br><br> 
          </p>
         <div class="item">
          <!-- Your image here -->
          
          <div style="text-align: center;">
            <img src="static/images/gif_9.giff" alt="MY ALT TEXT" style="width: 75%; margin: auto; display: block;"/>
              <div style="width: 75%; text-align: center; margin: 0 auto;">
                  Figure2: Examples of game instructions. Each instruction consists of three sections: game description, game trajectory, and game guidance (including action, language guidance, and the position of key elements).
              </div>
          </div> 
          
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper introduction -->

<!-- Paper Method -->
<section class="section hero is-white">
  <div class="container is-max-widescreen">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Decision Transformer with Game Instruction</h2>
        <div class="content has-text-justified">
          <p>
            The current section introduces the Decision Transformer with Game Instruction (DTGI), a DT model that integrates multimodal game instructions, as depicted in Figure 3. Firstly, we undertake the representation of multimodal instructions. Secondly, we calculate importance scores for each instruction in the Instruction set. Finally, We propose a novel design named SHyperGenerator to integrate game instructions into DT. N instruction generates n module parameters through hypernetworks. The module parameters are weighted based on the importance score of the instruction, and then utilized as adapter parameters.<br><br> 
          </p>
         <div class="item">
          <!-- Your image here -->
          <div style="width: 75%; margin: auto; text-align: center;">
              <!-- Your image here -->
              <img src="static/images/method.jpg" alt="MY ALT TEXT" style="width: 100%; display: block;"/>
              <div style="width: 100%; ">
                  Figure3: Model architecture of Decision Transformer with Game Instruction
              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper introduction -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{Jin2024,
  title={Read to Play (R2-Play): Decision Transformer with Multimodal Game Instruction},
  author={Yonggang Jin, Ge Zhang, Hao Zhao, Tianyu Zheng, Jiawei Guo, Liuyu Xiang, Shawn Yue, Stephen W. Huang, Wenhu Chen, Zhaofeng He, Jie Fu},
  journal={arXiv preprint arXiv:2402.04154},     
  year={2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">


        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
